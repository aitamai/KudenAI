# 03_データフロー（現行システム）

```
UI(PWA/モバイル)
   │
   ▼
Django API ── Redis(cache/OTP/session/day-buffer)
   │                    │
   │                    └─ Celery (夜間バッチ/ETL)
   │
   ├─ PostgreSQL (本番DB)
   ├─ Pinecone (embeddings)
   ├─ LangChain → LLM(OpenAI/Gemini/Claude)
   ├─ Whisper(STT)
   ├─ Stripe(Webhook)
   └─ Langfuse(Trace/Usage)
```

## 1. 認証/登録フロー
1. ユーザー入力（登録/ログイン）→ Django がメール/パスワードを検証
2. 成功時に Redis へ OTP を書き込み、メールで送信
3. OTP 検証後に `users` / `profiles` へ登録・更新、`session:{user_id}` を発行
4. 監査ログ + Langfuse trace に記録し、CloudWatch へメトリクス送信

## 2. 日記（チャットUI）データフロー
1. ダッシュボードから「日記」モードを選択 → AI が今日の質問を生成（LangChain + RAG）
2. ユーザー回答（テキスト/音声）→ 音声は Whisper で STT → 質問/回答ペアを Redis の `diary:buffer:{user_id}:{date}` に保存
3. 即時サマリ要求時は LangChain が既存日記を Pinecone で検索 → LLM が要約/追記提案を返却
4. 会話履歴は `conversations` / `messages` に書き込み、`diary_entries` とリンク
5. 保存確定時に Celery がバッファから PostgreSQL へ永続化し、Langfuse trace + token_ledger 記帳を実施

## 3. Embedding / RAG
1. 日記・性格診断・AI会話ログが DB に保存されるたびに Celery がジョブ発行
2. LangChain Embedding モジュールでベクトル生成 → `id = hash(user_id + text)` で Pinecone に upsert（重複排除）
3. Langfuse trace に「embedding」「pinecone_upsert」イベントを記録
4. RAG 実行時は `rag_sources` の権限/公開設定を確認し、引用スニペットを `messages.source_refs_json` に保存

## 4. Token / 課金データ
1. LLM 応答完了で `actual_tokens` を算出
2. Django で残量判定 → OK なら DB トランザクション内で
   - `token_ledger` にユーザー→運営、運営→アバターの仕訳を記録
   - `usage_counters` に日次ロールアップ
3. Celery が Stripe との照合、Langfuse へのコスト記録、BigQuery/Redshift へのストリーミングを担当
4. ユーザーはダッシュボードで利用量を照会（UI→API→PostgreSQL/BigQuery）

## 5. ナイトリープロセス
- 毎日 03:00 に Celery Beat が Redis バッファ（会話・日記）を PostgreSQL へフラッシュ
- 同時に Pinecone との整合チェック、Langfuse 集計、S3 へのバックアップを実行
- 異常時は再試行 & Slack 通知

## 6. 観測/監査データ
- Langfuse: LLM/RAG/Embedding の各フェーズで trace を記録し、token_ledger ID を添付
- CloudWatch/Sentry: API レイテンシ、OTP 失敗、LLM エラーを監視
- audit_logs/admin_events: 2FA 設定変更、AI 公開設定、トークン調整、プライバシー項目の更新を記録

